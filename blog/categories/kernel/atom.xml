<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel | Paris Blog]]></title>
  <link href="http://pyeh.github.io/blog/categories/kernel/atom.xml" rel="self"/>
  <link href="http://pyeh.github.io/"/>
  <updated>2014-12-29T00:34:10+08:00</updated>
  <id>http://pyeh.github.io/</id>
  <author>
    <name><![CDATA[Han-Chun Yeh (Paris)]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Android vibrator on MSM8909 platform]]></title>
    <link href="http://pyeh.github.io/blog/2014/12/22/android-vibrator-on-msm8909-platform/"/>
    <updated>2014-12-22T23:38:00+08:00</updated>
    <id>http://pyeh.github.io/blog/2014/12/22/android-vibrator-on-msm8909-platform</id>
    <content type="html"><![CDATA[<p>QPNP (Qualcomm Plug Plug N Play) vibrator is a peripheral on Qualcomm PMICs. The PMIC is connected to MSM8909 via SPMI bus.
Its driver uses the timed-output framework to interface with the userspace.</p>

<h3>1. MSM8909 Vibrator driver implementation and DTS file</h3>

<p>The vibrator is connected on the VIB_DRV_N line of the QPNP PMIC, and the PMIC vibrator driver can be used to program the voltage from 1.2 V to 3.1 V in 100 mV step through VIB_DRV_N pin.</p>

<h4>DTSI documentation</h4>

<p>``` c kernel/arch/arm/boot/dts/qcom/msm-pm8909.dtsi</p>

<pre><code>            pm8909_vib: qcom,vibrator@c000 {
                    compatible = "qcom,qpnp-vibrator";
                    reg = &lt;0xc000 0x100&gt;;
                    label = "vibrator";
                    status = "disabled";
            };
</code></pre>

<p>```</p>

<h4>Documentation/devicetree/bindings/platform/msm/qpnp-vibrator.txt</h4>

<h5>Required Properties:</h5>

<pre><code>- status: default status is set to "disabled.  Must be "okay"  
- compatible: must be "qcom,qpnp-vibrator"  
- label: name which describes the device  
- reg: address of device  
</code></pre>

<h5>Optional Properties:</h5>

<pre><code>- qcom,vib-timeout-ms: timeout of vibrator, in ms.  Default 15000 ms  
- qcom,vib-vtg-level-mV: voltage level, in mV.  Default 3100 mV  
</code></pre>

<h4>vibrator driver source</h4>

<p>QPNP vibrator operates in two modes &ndash; manual and PWM (Pulse Width Modulation). PWM is supported
through one of dtest lines connected to the vibrator. Manual mode is used on MSM8909 platform.</p>

<p>``` c qpnp_vibrator_prboe() under kernel/drivers/platform/msm/qpnp-vibrator.c
static int qpnp_vibrator_probe(struct spmi_device *spmi)
{</p>

<pre><code>    struct qpnp_vib *vib;
    struct resource *vib_resource;
    int rc;

    vib = devm_kzalloc(&amp;spmi-&gt;dev, sizeof(*vib), GFP_KERNEL);
    if (!vib)
            return -ENOMEM;

    vib-&gt;spmi = spmi;

    vib_resource = spmi_get_resource(spmi, 0, IORESOURCE_MEM, 0);
    if (!vib_resource) {
            dev_err(&amp;spmi-&gt;dev, "Unable to get vibrator base address\n");
            return -EINVAL;
    }
    vib-&gt;base = vib_resource-&gt;start;

    rc = qpnp_vib_parse_dt(vib);
    if (rc) {
            dev_err(&amp;spmi-&gt;dev, "DT parsing failed\n");
            return rc;
    }

    rc = qpnp_vibrator_config(vib);
    if (rc) {
            dev_err(&amp;spmi-&gt;dev, "vib config failed\n");
            return rc;
    }

    mutex_init(&amp;vib-&gt;lock);
    INIT_WORK(&amp;vib-&gt;work, qpnp_vib_update);

    hrtimer_init(&amp;vib-&gt;vib_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
    vib-&gt;vib_timer.function = qpnp_vib_timer_func;

    vib-&gt;timed_dev.name = "vibrator";
    vib-&gt;timed_dev.get_time = qpnp_vib_get_time;
    vib-&gt;timed_dev.enable = qpnp_vib_enable;

    dev_set_drvdata(&amp;spmi-&gt;dev, vib);

    rc = timed_output_dev_register(&amp;vib-&gt;timed_dev);
    if (rc &lt; 0)
            return rc;

    return rc;
</code></pre>

<p>}
```</p>

<h4>Uses timed output framework</h4>

<p>callback <em>.enable</em> and <em>.get_time</em> of <em>struct timed_output_dev</em> is hooked by <code>qpnp_vib_enable()</code> and <code>qpnp_vib_get_time()</code>.
``` c Implementation of qnp_vib_enable() and qpnp_vib_get_time()
static void qpnp_vib_enable(struct timed_output_dev *dev, int value)
{</p>

<pre><code>    struct qpnp_vib *vib = container_of(dev, struct qpnp_vib,
                                     timed_dev);

    mutex_lock(&amp;vib-&gt;lock);
    hrtimer_cancel(&amp;vib-&gt;vib_timer);

    if (value == 0)
            vib-&gt;state = 0;
    else {
            value = (value &gt; vib-&gt;timeout ?
                             vib-&gt;timeout : value);
            vib-&gt;state = 1;
            hrtimer_start(&amp;vib-&gt;vib_timer,
                          ktime_set(value / 1000, (value % 1000) * 1000000),
                          HRTIMER_MODE_REL);
    }
    mutex_unlock(&amp;vib-&gt;lock);
    schedule_work(&amp;vib-&gt;work);
</code></pre>

<p>}</p>

<p>static int qpnp_vib_get_time(struct timed_output_dev *dev)
{</p>

<pre><code>    struct qpnp_vib *vib = container_of(dev, struct qpnp_vib,
                                                     timed_dev);

    if (hrtimer_active(&amp;vib-&gt;vib_timer)) {
            ktime_t r = hrtimer_get_remaining(&amp;vib-&gt;vib_timer);
            return (int)ktime_to_us(r);
    } else
            return 0;
</code></pre>

<p>}
```</p>

<h4>wq is scheduled to execute work of qpnp_vib_update() and turns off vibrator via qpnp_vibrator_suspend() at suspend callback.</h4>

<p>Inside <code>qpnp_vib_update()</code> and <code>qpnp_vibrator_suspend()</code>, they both call <code>qpnp_vib_set()</code> to turn on/off vibrator.
``` c Implementation of qpnp_vib_set() to control vibrator via PWM or Manual Mode
static int qpnp_vib_set(struct qpnp_vib *vib, int on)
{</p>

<pre><code>    int rc;
    u8 val;

    if (on) {
            if (vib-&gt;mode != QPNP_VIB_MANUAL)
                    pwm_enable(vib-&gt;pwm_info.pwm_dev);
            else {
                    val = vib-&gt;reg_en_ctl;
                    val |= QPNP_VIB_EN;
                    rc = qpnp_vib_write_u8(vib, &amp;val,
                                    QPNP_VIB_EN_CTL(vib-&gt;base));
                    if (rc &lt; 0)
                            return rc;
                    vib-&gt;reg_en_ctl = val;
            }
    } else {
            if (vib-&gt;mode != QPNP_VIB_MANUAL)
                    pwm_disable(vib-&gt;pwm_info.pwm_dev);
            else {
                    val = vib-&gt;reg_en_ctl;
                    val &amp;= ~QPNP_VIB_EN;
                    rc = qpnp_vib_write_u8(vib, &amp;val,
                                    QPNP_VIB_EN_CTL(vib-&gt;base));
                    if (rc &lt; 0)
                            return rc;
                    vib-&gt;reg_en_ctl = val;
            }
    }

    return 0;
</code></pre>

<p>}
```</p>

<h3>2. Timed output class driver</h3>

<p>Timed output is a class drvier to allow changing a state and restore is automatically after a specfied timeout.
This exposes a user space interface under <em>/sys/class/timed_output/vibrator/enable</em> used by vibrator code.
``` c kernel/drivers/staging/android/timed_output.c
static ssize_t enable_show(struct device <em>dev, struct device_attribute </em>attr,</p>

<pre><code>            char *buf)
</code></pre>

<p>{</p>

<pre><code>    struct timed_output_dev *tdev = dev_get_drvdata(dev);
    int remaining = tdev-&gt;get_time(tdev);

    return sprintf(buf, "%d\n", remaining);
</code></pre>

<p>}</p>

<p>static ssize_t enable_store(</p>

<pre><code>            struct device *dev, struct device_attribute *attr,
            const char *buf, size_t size)
</code></pre>

<p>{</p>

<pre><code>    struct timed_output_dev *tdev = dev_get_drvdata(dev);
    int value;

    if (sscanf(buf, "%d", &amp;value) != 1)
            return -EINVAL;

    tdev-&gt;enable(tdev, value);

    return size;
</code></pre>

<p>}
int timed_output_dev_register(struct timed_output_dev *tdev)
{</p>

<pre><code>    int ret;

    if (!tdev || !tdev-&gt;name || !tdev-&gt;enable || !tdev-&gt;get_time)
            return -EINVAL;

    ret = create_timed_output_class();
    if (ret &lt; 0)
            return ret;

    tdev-&gt;index = atomic_inc_return(&amp;device_count);
    tdev-&gt;dev = device_create(timed_output_class, NULL,
            MKDEV(0, tdev-&gt;index), NULL, tdev-&gt;name);
    if (IS_ERR(tdev-&gt;dev))
            return PTR_ERR(tdev-&gt;dev);

    ret = device_create_file(tdev-&gt;dev, &amp;dev_attr_enable);
    if (ret &lt; 0)
            goto err_create_file;

    dev_set_drvdata(tdev-&gt;dev, tdev);
    tdev-&gt;state = 0;
    return 0;
</code></pre>

<p>err_create_file:</p>

<pre><code>    device_destroy(timed_output_class, MKDEV(0, tdev-&gt;index));
    pr_err("failed to register driver %s\n",
                    tdev-&gt;name);

    return ret;
</code></pre>

<p>}
EXPORT_SYMBOL_GPL(timed_output_dev_register);
```</p>

<h3>3. Android vibrator HAL</h3>

<p>Its interface to linux device drivers call with the specified timeout in millisecond via <code>vibrator_on()</code>.</p>

<p>``` c hardware/libhardware_legacy/vibrator/vibrator.c</p>

<h1>define THE_DEVICE &ldquo;/sys/class/timed_output/vibrator/enable&rdquo;</h1>

<p>static int sendit(int timeout_ms)
{</p>

<pre><code>int nwr, ret, fd;
char value[20];
</code></pre>

<h1>ifdef QEMU_HARDWARE</h1>

<pre><code>if (qemu_check()) {
    return qemu_control_command( "vibrator:%d", timeout_ms );
}
</code></pre>

<h1>endif</h1>

<pre><code>fd = open(THE_DEVICE, O_RDWR);
if(fd &lt; 0)
    return errno;

nwr = sprintf(value, "%d\n", timeout_ms);
ret = write(fd, value, nwr);

close(fd);

return (ret == nwr) ? 0 : -1;
</code></pre>

<p>}</p>

<p>int vibrator_on(int timeout_ms)
{</p>

<pre><code>/* constant on, up to maximum allowed time */
return sendit(timeout_ms);
</code></pre>

<p>}</p>

<p>int vibrator_off()
{</p>

<pre><code>return sendit(0);
</code></pre>

<p>}
```</p>

<h3>4. Native layer through JNI between HAL and vibrator service</h3>

<p>Controls of vibrator service reaches via <code>vibratorOn()</code>, <code>vibratorOff()</code>, and <code>vibratorExists()</code>.</p>

<p>``` c frameworks//base/services/core/jni/com_android_server_VibratorService.cpp
static void vibratorOn(JNIEnv *env, jobject clazz, jlong timeout_ms)
{</p>

<pre><code>// ALOGI("vibratorOn\n");
vibrator_on(timeout_ms);
</code></pre>

<p>}</p>

<p>static void vibratorOff(JNIEnv *env, jobject clazz)
{</p>

<pre><code>// ALOGI("vibratorOff\n");
vibrator_off();
</code></pre>

<p>}</p>

<p>static JNINativeMethod method_table[] = {</p>

<pre><code>{ "vibratorExists", "()Z", (void*)vibratorExists },
{ "vibratorOn", "(J)V", (void*)vibratorOn },
{ "vibratorOff", "()V", (void*)vibratorOff }
</code></pre>

<p>};</p>

<p>int register_android_server_VibratorService(JNIEnv *env)
{</p>

<pre><code>return jniRegisterNativeMethods(env, "com/android/server/VibratorService",
        method_table, NELEM(method_table));
</code></pre>

<p>}
```</p>

<h3>5. Java Vibrator Service</h3>

<p>In application layer before controling vibrator, applications have to get the access to the vibrator service.
The control goes into the vibrator service (<em>Framework Layer</em>) thru binder inteface.
For example, App. creates Vibrator object and start the vibration via <code>startVibrationLocked(vib)</code>.</p>

<p>Inside <code>startVibrationLocked()</code>:</p>

<p>If mTimeout != 0, then call the JNI function <code>vibratorOn()</code>.
else, call the code, which handle the rhythmic vibration pattern, which intern call the <code>vibratorOn()</code> through <code>VibrateThread()</code>.</p>

<p>``` java frameworks/base/services/core/java/com/android/server/VibratorService.java</p>

<pre><code>// Lock held on mVibrations
private void startVibrationLocked(final Vibration vib) {
    try {
        if (mLowPowerMode
                &amp;&amp; vib.mUsageHint != AudioAttributes.USAGE_NOTIFICATION_RINGTONE) {
            return;
        }

        int mode = mAppOpsService.checkAudioOperation(AppOpsManager.OP_VIBRATE,
                vib.mUsageHint, vib.mUid, vib.mOpPkg);
        if (mode == AppOpsManager.MODE_ALLOWED) {
            mode = mAppOpsService.startOperation(AppOpsManager.getToken(mAppOpsService),
                AppOpsManager.OP_VIBRATE, vib.mUid, vib.mOpPkg);
        }
        if (mode != AppOpsManager.MODE_ALLOWED) {
            if (mode == AppOpsManager.MODE_ERRORED) {
                Slog.w(TAG, "Would be an error: vibrate from uid " + vib.mUid);
            }
            mH.post(mVibrationRunnable);
            return;
        }
    } catch (RemoteException e) {
    }
    if (vib.mTimeout != 0) {
        doVibratorOn(vib.mTimeout, vib.mUid, vib.mUsageHint);
        mH.postDelayed(mVibrationRunnable, vib.mTimeout);
    } else {
        // mThread better be null here. doCancelVibrate should always be
        // called before startNextVibrationLocked or startVibrationLocked.
        mThread = new VibrateThread(vib);
        mThread.start();
    }
}

private void doVibratorOn(long millis, int uid, int usageHint) {
    synchronized (mInputDeviceVibrators) {
        if (DEBUG) {
            Slog.d(TAG, "Turning vibrator on for " + millis + " ms.");
        }
        try {
            mBatteryStatsService.noteVibratorOn(uid, millis);
            mCurVibUid = uid;
        } catch (RemoteException e) {
        }
        final int vibratorCount = mInputDeviceVibrators.size();
        if (vibratorCount != 0) {
            final AudioAttributes attributes = new AudioAttributes.Builder().setUsage(usageHint)
                    .build();
            for (int i = 0; i &lt; vibratorCount; i++) {
                mInputDeviceVibrators.get(i).vibrate(millis, attributes);
            }
        } else {
            vibratorOn(millis);
        }
    }
}
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes of debugging the kernel using Ftrace]]></title>
    <link href="http://pyeh.github.io/blog/2014/10/07/notes-of-debugging-the-kernel-using-ftrace/"/>
    <updated>2014-10-07T23:14:00+08:00</updated>
    <id>http://pyeh.github.io/blog/2014/10/07/notes-of-debugging-the-kernel-using-ftrace</id>
    <content type="html"><![CDATA[<p>This page has my notes for debugging the kernel using Ftrace</p>

<p>Ftrace is a tracing utility built directly into the Linux kernel. Ftrace was introduced in kernel 2.6.27 by Steven Rostedy and Ingo Molnar. It comes with its own ring buffer for storing trace data, and uses the GCC profiling mechanism. Documentation for this can be found in the Linux kernel source tree at <a href="https://www.kernel.org/doc/Documentation/trace/ftrace.txt">Documentation/trace/ftrace.txt</a></p>

<h4>References:</h4>

<ul>
<li><a href="http://lwn.net/Articles/365835/">Debugging the kernel using Ftrace &ndash; part 1</a></li>
<li><a href="http://lwn.net/Articles/366796/">Debugging the kernel using Ftrace &ndash; part 2</a></li>
<li><a href="http://lwn.net/Articles/370423/">Secrets of the Ftrace function tracer</a></li>
</ul>


<h3>Setting up Ftrace:</h3>

<p>When Ftrace is configured, it will create its own directory called tracing within the debugfs file system.</p>

<pre><code>[~]# cd /sys/kernel/debug/tracing
[tracing]#
</code></pre>

<p>For the purpose of debugging, the kernel configuration parameters that should be enabled are:<br/>
*  Kernel Function Tracer (FUNCTION_TRACER)<br/>
*  Kernel Function Graph Tracer (FUNCTION_GRAPH_TRACER)<br/>
*  Enable/disable ftrace dynamically (DYNAMIC_FTRACE)</p>

<p>The function tracer uses the <em>-pg</em> of <em>gcc</em> to have every function in the kernel call a special funciton <code>mcount()</code>.
During the compilation the mcount() call-sites are recorded, and that list is used at boot time to convert those
call-sites to NOP when CONFIG_DYNAMIC_FTRACE is set.
When the function or function graph tracer is enabled, that list is saved to convert those call-sites back to trace calls.</p>

<p>To find out which tracers are available, simply cat the <em>available_tracers</em> file in the tracing directory:</p>

<pre><code>root@K015:/ # cat /d/tracing/available_tracers
blk function_graph wakeup_rt wakeup preemptirqsoff preemptoff irqsoff function nop
</code></pre>

<p>To enable the function tracer, just echo &ldquo;function&rdquo; into the <em>current_tracer</em> file.</p>

<pre><code>root@K015:/d/tracing # echo function &gt; current_tracer
root@K015:/d/tracing # cat&gt; tracer
# tracer: function
#
# entries-in-buffer/entries-written: 205147/172709617   #P:4
#
#                              _-----=&gt; irqs-off
#                             / _----=&gt; need-resched
#                            | / _---=&gt; hardirq/softirq
#                            || / _--=&gt; preempt-depth
#                            ||| /     delay
#           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION
#              | |       |   ||||       |         |
        Binder_D-4038  [003] ....    98.468143: cgroup_tasks_write &lt;-cgroup_file_write
        Binder_D-4038  [003] ....    98.468143: attach_task_by_pid &lt;-cgroup_tasks_write
        Binder_D-4038  [003] ....    98.468143: cgroup_lock_live_group &lt;-attach_task_by_pid
        Binder_D-4038  [003] ....    98.468144: mutex_lock &lt;-cgroup_lock_live_group
        Binder_D-4038  [003] ....    98.468144: __might_sleep &lt;-mutex_lock
        Binder_D-4038  [003] ....    98.468144: __mutex_lock_slowpath &lt;-mutex_lock
        Binder_D-4038  [003] ....    98.468144: add_preempt_count &lt;-__mutex_lock_slowpath
        Binder_D-4038  [003] ...1    98.468145: sub_preempt_count &lt;-__mutex_lock_slowpath
        Binder_D-4038  [003] ....    98.468145: __rcu_read_lock &lt;-attach_task_by_pid
</code></pre>

<p>A header is printed with the tracer name that is represented by
the trace. In this case the tracer is &ldquo;function&rdquo;. Then it shows the
number of events in the buffer as well as the total number of entries
that were written. The difference is the number of entries that were
lost due to the buffer filling up (172709617 &ndash; 205147 = 172504407 events
lost). #P is the number of online CPUs (#P:4).</p>

<p>The header explains the content of the events. Task name &ldquo;Binder_D&rdquo;, the task
PID &ldquo;4038&rdquo;, the CPU that it was running on &ldquo;003&rdquo;, the latency format
(explained below), the timestamp in <secs>.<usecs> format, the
function name that was traced &ldquo;cgroup_tasks_write&rdquo; and the parent function that
called this function &ldquo;cgroup_file_write&rdquo;. The timestamp is the time
at which the function was entered.</p>

<pre><code>irqs-off: 'd' interrupts are disabled. '.' otherwise.
    Note: If the architecture does not support a way to
      read the irq flags variable, an 'X' will always
      be printed here.

need-resched:
'N' both TIF_NEED_RESCHED and PREEMPT_NEED_RESCHED is set,
'n' only TIF_NEED_RESCHED is set,
'p' only PREEMPT_NEED_RESCHED is set,
'.' otherwise.

hardirq/softirq:
'H' - hard irq occurred inside a softirq.
'h' - hard irq is running
's' - soft irq is running
'.' - normal context.

preempt-depth: The level of preempt_disabled
</code></pre>

<p>The above is mostly meaningful for kernel developers.</p>

<h3>Tips</h3>

<hr />

<h4>Using <em>trace_prink()</em></h4>

<p>If you are debugging a high volume area such as the timer interrupt, the scheduler, or the network, <em>printk()</em> can lead to bogging down the system or can even create a live lock.
It is also quite common to see a bug &ldquo;disappear&rdquo; when adding a few <em>printk()</em>s. This is due to the sheer overhead that <em>printk()</em> introduces.</p>

<p>Ftrace introduces a new form of <em>printk()</em> called <em>trace_printk()</em>.</p>

<p>For example, below code add entry/exit checkpoints to know how long system stays at standy mode
``` c arch/x86/platform/intel-mid/intel_soc_pmu.c
static int mid_suspend_enter(suspend_state_t state)
{</p>

<pre><code>    int ret;

    if (state != PM_SUSPEND_MEM)
            return -EINVAL;
</code></pre>

<p>&hellip;<SKIP>&hellip;</p>

<pre><code>    trace_printk("s3_entry\n");
    ret = standby_enter();
    trace_printk("s3_exit %d\n", ret);
</code></pre>

<p>&hellip;<SKIP>&hellip;</p>

<pre><code>    return ret;
</code></pre>

<p>}
```
<em>trace_printk()</em> output will appear in any tracer, even the function and function graph tracers.</p>

<pre><code># tracer: nop
#
# entries-in-buffer/entries-written: 4/4   #P:4
#
#                              _-----=&gt; irqs-off
#                             / _----=&gt; need-resched
#                            | / _---=&gt; hardirq/softirq
#                            || / _--=&gt; preempt-depth
#                            ||| /     delay
#           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION
#              | |       |   ||||       |         |
    kworker/u8:5-1178  [000] d...   318.726581: mid_suspend_enter: s3_entry
    kworker/u8:5-1178  [000] d...   377.664365: mid_suspend_enter: s3_exit 0
    kworker/u8:5-1178  [000] d...   378.514933: mid_suspend_enter: s3_entry
    kworker/u8:5-1178  [000] d...   569.010863: mid_suspend_enter: s3_exit 0
</code></pre>

<h4>Tracing a specific process</h4>

<p>Trace a specific process, or set of processes. The file set_ftrace_pid lets you specify specific processes that you want to trace.</p>

<pre><code>[tracing]# echo $$ &gt; set_ftrace_pid
</code></pre>

<p>The above will set the function tracer to only trace the bash shell that executed the echo command.</p>

<p>Clear the <em>set_ftrace_pid file</em> if you want to go back to generic function tracing</p>

<pre><code>[tracing]# echo -1 &gt; set_ftrace_pid
</code></pre>

<h4>Capturing Ftrace to oops when kernel panic</h4>

<p>You can capture the function calls leading up to a panic by placing the following on the kernel command line</p>

<pre><code>ftrace=function ftrace_dump_on_oops
</code></pre>

<p>or,  by echoing a &ldquo;1&rdquo; into <em>/proc/sys/kernel/ftrace_dump_on_oops</em>, will enable Ftrace to dump to the console the entire trace buffer in ASCII format on oops or panic.</p>

<h4>Find latencies on kernel startup</h4>

<p>Use the following on the kernel command line:</p>

<pre><code>tracing_thresh=10000 ftrace=function_graph
</code></pre>

<p>this traces all functions taking longer than 2000 microseconds (10 ms).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Intel_mide: CPUIDLE: Intel_idle processor driver and new C-States]]></title>
    <link href="http://pyeh.github.io/blog/2013/12/06/intel-mide-cpuidle-intel-idle-processor-driver-and-new-c-states/"/>
    <updated>2013-12-06T17:34:00+08:00</updated>
    <id>http://pyeh.github.io/blog/2013/12/06/intel-mide-cpuidle-intel-idle-processor-driver-and-new-c-states</id>
    <content type="html"><![CDATA[<p>When there is nothing left to do, the CPUs will go into the idle state to wait until it is needed again. These idle modes, which go by names like &ldquo;C states,&rdquo; vary in the amount of power saved, but also in the amount of ancillary information which may be lost and the amount of time required to get back into a fully-functional mode.</p>

<p>Intel_idle driver actually performs idle state power management, and regsiters into existing CPU Idle subsystem and extends driver for Merrifield CPU (Slivermont). It also introduces a new platform idle states C7x deeper than traditional C6 state. The overall idea is that CPU C7x-states are extended to devices and rest of the platform, hence puting the platform to S0ix states.</p>

<p>On intel mid platform (Merrifield), the PMU driver communicates with the Intel_idle driver, platform device drivers, pci drivers, and the PMU firmwares (NC: Punit, SC:SCU) to coordinate platform power state transitions.</p>

<ol>
<li><p>The PMU driver provides a platform-specific callback to the Intel_idle driver so that long periods of idleness can be extended to the entire platform.</p>

<ul>
<li>soc_s0ix_idle()&ndash;>enter_s0ix_state() // intel_idle driver defined at /drivers/idle/intel_idle.c</li>
<li>mid_s0ix_enter()            // PMU driver defined at /arch/x86/platform/intel-mid/intel_soc_pmu.c</li>
</ul>


<p> I could find out cpuidle_state->.enter() associated with soc_s0ix_idle() for C6 state on medfield platform, and the call stack looks like as above shown. However, I don&rsquo;t figure out the similar assoication between intel_idle and pmu driver on merrifield platform. I am curious if this statement is also appliciable on merrifield platform ??</p></li>
<li><p>Based on hint of idleness, the PMU driver extends CPU idleness to the reset of the platform via standard Linux PM_QoS, Runtime PM, and PCI PM calls.</p></li>
<li><p>Once the CPU and devices are all idle, the PMU driver programs the North and South Complex PMUs to implement the required power transitiion for S0ix to eumlate C7-x states.</p></li>
</ol>


<p>Here is very good reference to understand the cpuidle governor and subsystem
<a href="http://lwn.net/Articles/384146/">http://lwn.net/Articles/384146/</a> before digging into how intel_idle driver fits with current cpuilde intrastructure.</p>

<p>The below is code trace for intel_idle driver located at &ldquo;/drivers/idle/intel_idle.c&rdquo;
&ndash; Comment in intel_idle driver
<code>
/*
 * intel_idle is a cpuidle driver that loads on specific Intel processors
 * in lieu of the legacy ACPI processor_idle driver.  The intent is to
 * make Linux more efficient on these processors, as intel_idle knows
 * more than ACPI, as well as make Linux more immune to ACPI BIOS bugs.
 */
</code>
&ndash; Driver Init.</p>

<pre><code>* intel_idle_probe() starts to match current CPU with array of x86_cpu_ids via and assign corresponding default cpuidle C-state table  
* intel_idle_cpuidle_driver_init() checks real C-state table and update its state when needed (eg, target_residency)
* register the intel_dile driver with the cpudile subsystem through cpuidle_register_driver()  
* intel_idle_cpu_init() allocates, initializes, and registers cpuidle_device for each CPU
* register cpu_hotplug_notifer to know about CPUs going up/dow
</code></pre>

<p>``` c intel_idle_init()
static int __init intel_idle_init(void)                                                                                           <br/>
{</p>

<pre><code>    int retval, i;

    /* Do not load intel_idle at all for now if idle= is passed */
    if (boot_option_idle_override != IDLE_NO_OVERRIDE)
            return -ENODEV;

    retval = intel_idle_probe();
    if (retval)
            return retval;

    intel_idle_cpuidle_driver_init();
    retval = cpuidle_register_driver(&amp;intel_idle_driver);
    if (retval) {
            struct cpuidle_driver *drv = cpuidle_get_driver();
            printk(KERN_DEBUG PREFIX "intel_idle yielding to %s",
                    drv ? drv-&gt;name : "none");
            return retval;
    }

    intel_idle_cpuidle_devices = alloc_percpu(struct cpuidle_device);
    if (intel_idle_cpuidle_devices == NULL)
            return -ENOMEM;

    for_each_online_cpu(i) {
            retval = intel_idle_cpu_init(i);
            if (retval) {
                    cpuidle_unregister_driver(&amp;intel_idle_driver);
                    return retval;
            }

            if (platform_is(INTEL_ATOM_BYT)) {
                    /* Disable automatic core C6 demotion by PUNIT */
                    if (wrmsr_on_cpu(i, CLPU_CR_C6_POLICY_CONFIG,
                                    DISABLE_CORE_C6_DEMOTION, 0x0))
                            pr_err("Error to disable core C6 demotion");
                    /* Disable automatic module C6 demotion by PUNIT */
                    if (wrmsr_on_cpu(i, CLPU_MD_C6_POLICY_CONFIG,
                                    DISABLE_MODULE_C6_DEMOTION, 0x0))
                            pr_err("Error to disable module C6 demotion");
            }
    }
    register_cpu_notifier(&amp;cpu_hotplug_notifier);

    return 0;
</code></pre>

<p>}
```</p>

<ul>
<li>Default cpuidle C-states for merrifield

<ul>
<li>&ldquo;flags&rdquo; field describes the characteristics of this sleep state

<ul>
<li>CPUIDLE_FLAG_TIME_VALID should be set if it is possible to accurately measure the amount of time spent in this particular idle state.</li>
<li>CPUIDLE_FLAG_TLB_FLUSHED is set to inidicate the HW flushes the TLB for this state.</li>
<li>MWAIT takes an 8-bit &ldquo;hint&rdquo; in EAX &ldquo;suggesting&rdquo; the C-state (top nibble) and sub-state (bottom nibble). 0x00 means &ldquo;MWAIT(C1)&rdquo;, 0x10 means &ldquo;MWAIT(C2)&rdquo; etc.</li>
</ul>
</li>
<li>&ldquo;exit_latency&rdquo; in US says how long it takes to get back to a fully functional state.</li>
<li>&ldquo;target_residency&rdquo; in US is the minimum amount of time the processor should spend in this state to make the transition worth the effort.</li>
<li>The &ldquo;enter()&rdquo; function will be called when the current governor decides to put the CPU into the given state<br/>
``` c Merrifield CPUidle C states

<h1>if defined(CONFIG_REMOVEME_INTEL_ATOM_MRFLD_POWER)</h1>

static struct cpuidle_state mrfld_cstates[CPUIDLE_STATE_MAX] = {
  { /<em> MWAIT C1 </em>/
          .name = &ldquo;C1-ATM&rdquo;,
          .desc = &ldquo;MWAIT 0x00&rdquo;,
          .flags = MWAIT2flg(0x00) | CPUIDLE_FLAG_TIME_VALID,
          .exit_latency = 1,
          .target_residency = 4,
          .enter = &amp;intel_idle },
  { /<em> MWAIT C4 </em>/
          .name = &ldquo;C4-ATM&rdquo;,
          .desc = &ldquo;MWAIT 0x30&rdquo;,
          .flags = MWAIT2flg(0x30) | CPUIDLE_FLAG_TIME_VALID | CPUIDLE_FLAG_TLB_FLUSHED,
          .exit_latency = 100,
          .target_residency = 400,
          .enter = &amp;intel_idle },
  { /<em> MWAIT C6 </em>/
          .name = &ldquo;C6-ATM&rdquo;,
          .desc = &ldquo;MWAIT 0x52&rdquo;,
          .flags = MWAIT2flg(0x52) | CPUIDLE_FLAG_TIME_VALID | CPUIDLE_FLAG_TLB_FLUSHED,
          .exit_latency = 140,
          .target_residency = 560,
          .enter = &amp;intel_idle },
  { /<em> MWAIT C7-S0i1 </em>/
          .name = &ldquo;S0i1-ATM&rdquo;,
          .desc = &ldquo;MWAIT 0x60&rdquo;,
          .flags = MWAIT2flg(0x60) | CPUIDLE_FLAG_TIME_VALID | CPUIDLE_FLAG_TLB_FLUSHED,
          .exit_latency = 1200,
          .target_residency = 4000,
          .enter = &amp;intel_idle },
  { /<em> MWAIT C9-S0i3 </em>/
          .name = &ldquo;S0i3-ATM&rdquo;,
          .desc = &ldquo;MWAIT 0x64&rdquo;,
          .flags = MWAIT2flg(0x64) | CPUIDLE_FLAG_TIME_VALID | CPUIDLE_FLAG_TLB_FLUSHED,
          .exit_latency = 10000,
          .target_residency = 20000,
          .enter = &amp;intel_idle },
  {
          .enter = NULL }
};

<h1>else</h1>

<h1>define mrfld_cstates atom_cstates</h1>

<h1>endif</h1></li>
</ul>
</li>
</ul>


<p>```
The actual performs given C-state transitions implemented by intel_idle(). As we saw, this is done through &ldquo;enter()&rdquo; functions associated with each state. The decision as to which idle state makes sense in a given situation is very much a policy issue implemented by the cpuidle &ldquo;governors&rdquo;.</p>

<p>A call to enter() is a request from the current governor to put the CPU associated with dev into the given state. Note that enter() is free to choose a different state if there is a good reason to do so, but it should store the actual state used in the device&rsquo;s last_state field.
``` c intel_idle
/<em><em>                                                                                                                               <br/>
 * intel_idle
 * @dev: cpuidle_device
 * @drv: cpuidle driver
 * @index: index of cpuidle state
 *
 * Must be called under local_irq_disable().
 </em>/
static int intel_idle(struct cpuidle_device </em>dev,</p>

<pre><code>            struct cpuidle_driver *drv, int index)
</code></pre>

<p>{</p>

<pre><code>    unsigned long ecx = 1; /* break on interrupt flag */
    struct cpuidle_state *state = &amp;drv-&gt;states[index];
    unsigned long eax = flg2MWAIT(state-&gt;flags);
    unsigned int cstate;
    int cpu = smp_processor_id();
</code></pre>

<h1>if (defined(CONFIG_REMOVEME_INTEL_ATOM_MRFLD_POWER) &amp;&amp; \</h1>

<pre><code>    defined(CONFIG_PM_DEBUG))
    {
            /* Get Cstate based on ignore table from PMU driver */
            unsigned int ncstate;
            cstate =
            (((eax) &gt;&gt; MWAIT_SUBSTATE_SIZE) &amp; MWAIT_CSTATE_MASK) + 1;
            ncstate = pmu_get_new_cstate(cstate, &amp;index);
            eax     = flg2MWAIT(drv-&gt;states[index].flags);
    }
</code></pre>

<h1>endif</h1>

<pre><code>    cstate = (((eax) &gt;&gt; MWAIT_SUBSTATE_SIZE) &amp; MWAIT_CSTATE_MASK) + 1;

    /*
     * leave_mm() to avoid costly and often unnecessary wakeups
     * for flushing the user TLB's associated with the active mm.
     */
    if (state-&gt;flags &amp; CPUIDLE_FLAG_TLB_FLUSHED)
            leave_mm(cpu);

    if (!(lapic_timer_reliable_states &amp; (1 &lt;&lt; (cstate))))
            clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &amp;cpu);

    if (!need_resched()) {

            __monitor((void *)&amp;current_thread_info()-&gt;flags, 0, 0);
            smp_mb();
            if (!need_resched())
                    __mwait(eax, ecx);
</code></pre>

<h1>if defined(CONFIG_REMOVEME_INTEL_ATOM_MDFLD_POWER) || \</h1>

<pre><code>    defined(CONFIG_REMOVEME_INTEL_ATOM_CLV_POWER)
            if (!need_resched() &amp;&amp; is_irq_pending() == 0)
                    __get_cpu_var(update_buckets) = 0;
</code></pre>

<h1>endif</h1>

<pre><code>    }

    if (!(lapic_timer_reliable_states &amp; (1 &lt;&lt; (cstate))))
            clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &amp;cpu);

    return index;
</code></pre>

<p>}</p>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Intel_mid: Android power management flow for suspend/resume using S0i3 implementation]]></title>
    <link href="http://pyeh.github.io/blog/2013/12/04/intel-mid-android-power-management-flow-for-suspend-slash-resume-using-s0i3-implementation/"/>
    <updated>2013-12-04T11:19:00+08:00</updated>
    <id>http://pyeh.github.io/blog/2013/12/04/intel-mid-android-power-management-flow-for-suspend-slash-resume-using-s0i3-implementation</id>
    <content type="html"><![CDATA[<p>The main PMU driver (arch/x86/platform/intel-mid/intel_soc_pmu.c) hooks to support Linux PM suspend/resume flows as follows.<br/>
The S0ix states are low-power active idle states that platform can be transitioned into.
&ndash; Register PMU driver as PCI device<br/>
The PMU driver registers mid_suspend_ops via suspend_set_ops().
```c mid_pci_register_init
/<em>*
 * mid_pci_register_init &ndash; register the PMU driver as PCI device
 </em>/
static struct pci_driver driver = {</p>

<pre><code>    .name = PMU_DRV_NAME,
    .id_table = mid_pm_ids,
    .probe = mid_pmu_probe,
    .remove = mid_pmu_remove,
    .shutdown = mid_pmu_shutdown
</code></pre>

<p>};</p>

<p>static int __init mid_pci_register_init(void)
{</p>

<pre><code>    int ret;

    mid_pmu_cxt = kzalloc(sizeof(struct mid_pmu_dev), GFP_KERNEL);

    if (mid_pmu_cxt == NULL)
            return -ENOMEM;

    mid_pmu_cxt-&gt;s3_restrict_qos =
            kzalloc(sizeof(struct pm_qos_request), GFP_KERNEL);
    if (mid_pmu_cxt-&gt;s3_restrict_qos) {
            pm_qos_add_request(mid_pmu_cxt-&gt;s3_restrict_qos,
                     PM_QOS_CPU_DMA_LATENCY, PM_QOS_DEFAULT_VALUE);
    } else {
            return -ENOMEM;
    }

    init_nc_device_states();

    mid_pmu_cxt-&gt;nc_restrict_qos =
            kzalloc(sizeof(struct pm_qos_request), GFP_KERNEL);
    if (mid_pmu_cxt-&gt;nc_restrict_qos == NULL)
            return -ENOMEM;

    /* initialize the semaphores */
    sema_init(&amp;mid_pmu_cxt-&gt;scu_ready_sem, 1);

    /* registering PCI device */
    ret = pci_register_driver(&amp;driver);
    suspend_set_ops(&amp;mid_suspend_ops);

    return ret;
</code></pre>

<p>}
<code>
- mid_suspend_enter() performs the required S3 state over standby_enter()  
check_nc_sc_status() hooked by mrfld_nc_sc_status_check (defind at arch/x86/platform/intel-mid/intel_soc_mrfld.c) is to check north complex (NC) and soutch complex (SC) device status. Return true if all NC and SC devices are in D0i3.
</code> c mid_suspend_enter()
static const struct platform_suspend_ops mid_suspend_ops = {</p>

<pre><code>    .begin = mid_suspend_begin,
    .valid = mid_suspend_valid,
    .prepare = mid_suspend_prepare,
    .prepare_late = mid_suspend_prepare_late,
    .enter = mid_suspend_enter,
    .end = mid_suspend_end,
</code></pre>

<p>};</p>

<p>static int mid_suspend_enter(suspend_state_t state)                                                                               <br/>
{</p>

<pre><code>    int ret;

    if (state != PM_SUSPEND_MEM)
            return -EINVAL;

    /* one last check before entering standby */
    if (pmu_ops-&gt;check_nc_sc_status) {
            if (!(pmu_ops-&gt;check_nc_sc_status())) {
                    trace_printk("Device d0ix status check failed! Aborting Standby entry!\n");
                    WARN_ON(1);
            }
    }

    trace_printk("s3_entry\n");
    ret = standby_enter();
    trace_printk("s3_exit %d\n", ret);
    if (ret != 0)
            dev_dbg(&amp;mid_pmu_cxt-&gt;pmu_dev-&gt;dev,
                            "Failed to enter S3 status: %d\n", ret);

    return ret;
</code></pre>

<p>}</p>

<p>```</p>

<ul>
<li>standby_enter() performs requried S3 state

<ul>
<li>mid_state_to_sys_state() maps power states to driver&rsquo;s internal indexes.</li>
<li>mid_s01x_enter() performs required S3 state</li>
<li>__mwait(mid_pmu_cxt->s3_hint, 1) is issued with a hint to enter S0i3(S3 emulation using S0i3, as I observed that MRFLD_S3_HINT with 0x64 is same as MID_S0I3_STATE with 0x64). When both core issue an mwait C7, it is a hint provided by the idle driver to enter an S0ix state.</li>
<li><p>This triggers S0i3 entry, but the decision and policy is selected by SCU FW.
``` c standby_enter()
static int standby_enter(void)
{
  u32 temp = 0;
  int s3_state = mid_state_to_sys_state(MID_S3_STATE);</p>

<p>  if (mid_s0ix_enter(MID_S3_STATE) != MID_S3_STATE) {
          pmu_set_s0ix_complete();
          return -EINVAL;
  }</p>

<p>  /<em> time stamp for end of s3 entry </em>/
  time_stamp_for_sleep_state_latency(s3_state, false, true);</p>

<p>  <strong>monitor((void *) &amp;temp, 0, 0);
  smp_mb();
  </strong>mwait(mid_pmu_cxt->s3_hint, 1);</p>

<p>  /<em> time stamp for start of s3 exit </em>/
  time_stamp_for_sleep_state_latency(s3_state, true, false);</p>

<p>  pmu_set_s0ix_complete();</p>

<p>  /<em>set wkc to appropriate value suitable for s0ix</em>/
  writel(mid_pmu_cxt->ss_config->wake_state.wake_enable[0],
                 &amp;mid_pmu_cxt->pmu_reg->pm_wkc[0]);
  writel(mid_pmu_cxt->ss_config->wake_state.wake_enable[1],
                 &amp;mid_pmu_cxt->pmu_reg->pm_wkc[1]);</p>

<p>  mid_pmu_cxt->camera_off = 0;
  mid_pmu_cxt->display_off = 0;</p>

<p>  if (platform_is(INTEL_ATOM_MRFLD))
          up(&amp;mid_pmu_cxt->scu_ready_sem);</p>

<p>  return 0;
}
```</p></li>
</ul>
</li>
<li>mid_s01x_enter()

<ul>
<li>pmu_prepare_wake() will mask wakeup from AONT timers for s3. If s3 is aborted for any reason, we don&rsquo;t want to leave AONT timers masked until next suspend, otherwise if next to happen is s0ix, no timer could wakeup SoC from s0ix and we might miss to kick the kernel watchdog.</li>
<li><p>enter() hooked by mrfld_pmu_enter (defined at arch/x86/platform/intel-mid/intel_soc_mrfld.c). Compared to Medfield and Covertail platform, PM_CMD is not required to send to SCU.
``` c mid_s01x_enter()
int mid_s0ix_enter(int s0ix_state)
{
  int ret = 0;</p>

<p>  if (unlikely(!pmu_ops || !pmu_ops->enter))
          goto ret;</p>

<p>  /* check if we can acquire scu_ready_sem</p>

<ul>
<li> if we are not able to then do a c6 */
if (down_trylock(&amp;mid_pmu_cxt->scu_ready_sem))
      goto ret;</li>
</ul>


<p>  /<em> If PMU is busy, we&rsquo;ll retry on next C6 </em>/
  if (unlikely(_pmu_read_status(PMU_BUSY_STATUS))) {
          up(&amp;mid_pmu_cxt->scu_ready_sem);
          pr_debug(&ldquo;mid_pmu_cxt->scu_read_sem is up\n&rdquo;);
          goto ret;
  }</p>

<p>  pmu_prepare_wake(s0ix_state);</p>

<p>  /<em> no need to proceed if schedule pending </em>/
  if (unlikely(need_resched())) {
          pmu_stat_clear();
          up(&amp;mid_pmu_cxt->scu_ready_sem);
          goto ret;
  }</p>

<p>  /<em> entry function for pmu driver ops </em>/
  if (pmu_ops->enter(s0ix_state))
          ret = s0ix_state;</p></li>
</ul>
</li>
</ul>


<p>ret:</p>

<pre><code>    return ret;
</code></pre>

<p>}</p>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Intel_mid: Introduction of android watchdogs and kernel watchdogs]]></title>
    <link href="http://pyeh.github.io/blog/2013/12/02/intel-mid-introduction-of-android-watchdog-and-kernel-watchdog/"/>
    <updated>2013-12-02T17:29:00+08:00</updated>
    <id>http://pyeh.github.io/blog/2013/12/02/intel-mid-introduction-of-android-watchdog-and-kernel-watchdog</id>
    <content type="html"><![CDATA[<p>Watchdogs monitor software and hardware device and prevent whole system from hanging. After looking into android BSP running on intel mid, merrifield, platform, I will try to classfiy watchdog into the following types. The first two belong to native Android supporting, and the last three are specified to intel-mid platform.</p>

<h2>1. Android framework&rsquo;s Java* watchdog</h2>

<p>Deal with cases when any of the following locks is held for more than a minute or when ServerThread is busy.</p>

<p>&mdash; ThermalManagerService<br/>
&mdash; PowerManagerService<br/>
&mdash; WindowMangerService<br/>
&mdash; MountService<br/>
&mdash; NetworkManagementService<br/>
&mdash; ActivityMangerService</p>

<p>If one of above services hangs for one minute, the java watchdog kills it and results in restarting android&rsquo;s framework by killing the SystemServer.</p>

<h2>2. Device-critical services</h2>

<p>Critical services are declared as &ldquo;critical&rdquo; in the corresponding rc files (eg, ueventd, servicemanager). If critical services exist or crash more than four times in four minutes, the device will reboot into recovery mode. This feature is handled by the init process.</p>

<h2>3. Kernel watchdog leads to COLD_RESET</h2>

<p>The kernel watchdog prvents the operating system from hanging. The System Control Unit (SCU firmware) resets the platform when the kernel cannot schedule the watchdog daemon (/usr/bin/ia_watchdogd).</p>

<p>The driver located at /drivers/watchdog/intel_scu_watchdog_evo.c provides a /dev/watchdog device to access the kernel watchdog and ioctls to configure the timer. Since the SCU provides the functionality, all access to watchdog features are routed to the SCU via an IPC (see more PIC regitrations at arch/x86/platform/intel-mid/intel_mid_scu.c)</p>

<ul>
<li>Init code installs the driver
``` c watchdog_rpmsg_init()
static struct rpmsg_driver watchdog_rpmsg = {
      .drv.name       = KBUILD_MODNAME,
      .drv.owner      = THIS_MODULE,
      .id_table       = watchdog_rpmsg_id_table,
      .probe          = watchdog_rpmsg_probe,
      .callback       = watchdog_rpmsg_cb,
      .remove         = watchdog_rpmsg_remove,
};</li>
</ul>


<p>static int __init watchdog_rpmsg_init(void)
{</p>

<pre><code>    if (intel_mid_identify_cpu() == INTEL_MID_CPU_CHIP_TANGIER)
            return register_rpmsg_driver(&amp;watchdog_rpmsg);
    else {                                                                                                                      
            pr_err("%s: watchdog driver: bad platform\n", __func__);
            return -ENODEV;
    }
</code></pre>

<p>}</p>

<h1>ifdef MODULE</h1>

<p>module_init(watchdog_rpmsg_init);</p>

<h1>else</h1>

<p>rootfs_initcall(watchdog_rpmsg_init);</p>

<h1>endif</h1>

<p>```</p>

<ul>
<li><p>create the /dev/watchdog only if the disabled_kernel_watchdog module parameter is not set. It gets the timer&rsquo;s configuration, registers reboot notifier, registers dump handler to irq#15, and adds sysfs/debugfs entries.
``` c watchdog_rpmsg_probe()&ndash;>intel_scu_watchdog_init()
/<em> Init code </em>/
static int intel_scu_watchdog_init(void)
{
      int ret = 0;</p>

<pre><code>  watchdog_device.normal_wd_action   = SCU_COLD_RESET_ON_TIMEOUT;
  watchdog_device.reboot_wd_action   = SCU_COLD_RESET_ON_TIMEOUT;
  watchdog_device.shutdown_wd_action = SCU_COLD_OFF_ON_TIMEOUT;
</code></pre></li>
</ul>


<h1>ifdef CONFIG_DEBUG_FS</h1>

<pre><code>    watchdog_device.panic_reboot_notifier = false;
</code></pre>

<h1>endif /<em> CONFIG_DEBUG_FS </em>/</h1>

<pre><code>    /* Initially, we are not in shutdown mode */
    watchdog_device.shutdown_flag = false;

    /* Check timeouts boot parameter */
    if (check_timeouts(pre_timeout, timeout)) {
            pr_err("%s: Invalid timeouts\n", __func__);
            return -EINVAL;
    }

    /* Reboot notifier */
    watchdog_device.reboot_notifier.notifier_call = reboot_notifier;
    watchdog_device.reboot_notifier.priority = 1;
    ret = register_reboot_notifier(&amp;watchdog_device.reboot_notifier);
    if (ret) {
            pr_crit("cannot register reboot notifier %d\n", ret);
            goto error_stop_timer;
    }

    /* Do not publish the watchdog device when disable (TO BE REMOVED) */
    if (!disable_kernel_watchdog) {
            watchdog_device.miscdev.minor = WATCHDOG_MINOR;
            watchdog_device.miscdev.name = "watchdog";
            watchdog_device.miscdev.fops = &amp;intel_scu_fops;

            ret = misc_register(&amp;watchdog_device.miscdev);
            watchdog_device.miscdev.fops = &amp;intel_scu_fops;

            ret = misc_register(&amp;watchdog_device.miscdev);
            if (ret) {
                    pr_crit("Cannot register miscdev %d err =%d\n",
                            WATCHDOG_MINOR, ret);
                    goto error_reboot_notifier;
            }
    }

    /* MSI #15 handler to dump registers */
    handle_mrfl_dev_ioapic(EXT_TIMER0_MSI);
    ret = request_irq((unsigned int)EXT_TIMER0_MSI,
            watchdog_warning_interrupt,
            IRQF_SHARED|IRQF_NO_SUSPEND, "watchdog",
            &amp;watchdog_device);
    if (ret) {
            pr_err("error requesting warning irq %d\n",
                   EXT_TIMER0_MSI);
            pr_err("error value returned is %d\n", ret);
            goto error_misc_register;
    }
</code></pre>

<h1>ifdef CONFIG_INTEL_SCU_SOFT_LOCKUP</h1>

<pre><code>    init_timer(&amp;softlock_timer);
</code></pre>

<h1>endif</h1>

<pre><code>    if (disable_kernel_watchdog) {
            pr_err("%s: Disable kernel watchdog\n", __func__);

            /* Make sure timer is stopped */
            ret = watchdog_stop();
            if (ret != 0)
                    pr_debug("cant disable timer\n");
    }
</code></pre>

<h1>ifdef CONFIG_DEBUG_FS</h1>

<pre><code>    ret = create_debugfs_entries();  
    if (ret) {
            pr_err("%s: Error creating debugfs entries\n", __func__);
            goto error_debugfs_entry;
    }
</code></pre>

<h1>endif</h1>

<pre><code>    watchdog_device.started = false;

    ret = create_watchdog_sysfs_files();
    if (ret) {
            pr_err("%s: Error creating debugfs entries\n", __func__);
            goto error_sysfs_entry;
    }

    return ret;
</code></pre>

<p>error_sysfs_entry:</p>

<pre><code>    /* Nothing special to do */
</code></pre>

<h1>ifdef CONFIG_DEBUG_FS</h1>

<p>error_debugfs_entry:</p>

<pre><code>    /* Remove entries done by create function */
</code></pre>

<h1>endif</h1>

<p>error_misc_register:</p>

<pre><code>    misc_deregister(&amp;watchdog_device.miscdev);
</code></pre>

<p>error_reboot_notifier:</p>

<pre><code>    unregister_reboot_notifier(&amp;watchdog_device.reboot_notifier);
</code></pre>

<p>error_stop_timer:</p>

<pre><code>    watchdog_stop();

    return ret;
</code></pre>

<p>}
```</p>

<ul>
<li><p>interrupt handler related to pre-timeout dumps kernel backtraces.
``` c watchdog_warning_interrupt
/<em> warning interrupt handler </em>/
static irqreturn_t watchdog_warning_interrupt(int irq, void *dev_id)
{
      pr_warn(&ldquo;[SHTDWN] %s, WATCHDOG TIMEOUT!\n&rdquo;, <strong>func</strong>);</p>

<pre><code>  /* Let's reset the platform after dumping some data */ 
  trigger_all_cpu_backtrace(); 
  panic("Kernel Watchdog"); 

  /* This code should not be reached */ 
  return IRQ_HANDLED; 
</code></pre>

<p>}</p></li>
</ul>


<p>```</p>

<ul>
<li><p>When power transisition happens, reboot_notifier is called for re-configuring watchdog timeouts and its default behavior.
COLD_RESET is set to reboot, and COLD_OFF is set to poewr halt and off. In case of a stucking rebooting or shutdown procedure, the platform will still could execute reset or power-off seperately.
``` c reboot_notifier
/<em> Reboot notifier </em>/
static int reboot_notifier(struct notifier_block <em>this,
                         unsigned long code,
                         void </em>another_unused)
{
      int ret;</p>

<pre><code>  if (code == SYS_RESTART || code == SYS_HALT || code == SYS_POWER_OFF) {
          pr_warn("Reboot notifier\n");

          if (watchdog_set_appropriate_timeouts())
                  pr_crit("reboot notifier cant set time\n");

          switch (code) {
          case SYS_RESTART:
                  ret = watchdog_set_reset_type(
                          watchdog_device.reboot_wd_action);
                  break;

          case SYS_HALT:
          case SYS_POWER_OFF:
                  ret = watchdog_set_reset_type(
                          watchdog_device.shutdown_wd_action);
                  break;
          }
          if (ret)
                  pr_err("%s: could not set reset type\n", __func__);
</code></pre></li>
</ul>


<h1>ifdef CONFIG_DEBUG_FS</h1>

<pre><code>            /* debugfs entry to generate a BUG during
            any shutdown/reboot call */
            if (watchdog_device.panic_reboot_notifier)
                    BUG();
</code></pre>

<h1>endif</h1>

<pre><code>            /* Don't do instant reset on close */
            reset_on_release = false;

            /* Kick once again */
            if (disable_kernel_watchdog == false) {
                    ret = watchdog_keepalive();
                    if (ret)
                            pr_warn("%s: no keep alive\n", __func__);

                    /* Don't allow any more keep-alives */
                    watchdog_device.shutdown_flag = true;
            }
    }
    return NOTIFY_DONE;
</code></pre>

<p>}
```</p>

<h2>4. Userspace watchdog daemon</h2>

<p>Source codes are located at /hardware/ia_watchdog/watchdog_daemon folder) and target location is /usr/bin/ia_watchdogd. This daemon is declared as one-shot service in the rc file (init.watchdog.rc) and perform the following steps:</p>

<p>&mdash; open the watchdog device /dev/watchdog.<br/>
&mdash; configure the pre_timeout with 75 seconds and timeout with 90 seconds.<br/>
&mdash; Loop forever. In the loop, kick the watchdog device (by writing to &lsquo;R&rsquo; to /dev/watchdog) every 60 seconds.</p>

<h2>5. SCU watchdog leads to PLATFORM_RESET(deep reset)</h2>

<p>This prevents the platform stucking on SCU by issuing a PLATFORM_RESET because the interface between the SCU and PMIC is broken.</p>
]]></content>
  </entry>
  
</feed>
