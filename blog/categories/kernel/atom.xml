<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel | Paris Blog]]></title>
  <link href="http://pyeh.github.io/blog/categories/kernel/atom.xml" rel="self"/>
  <link href="http://pyeh.github.io/"/>
  <updated>2013-12-09T19:16:24+08:00</updated>
  <id>http://pyeh.github.io/</id>
  <author>
    <name><![CDATA[Han-Chun Yeh (Paris)]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Intel_mide: CPUIDLE: Intel_idle processor driver and new C-States]]></title>
    <link href="http://pyeh.github.io/blog/2013/12/06/intel-mide-cpuidle-intel-idle-processor-driver-and-new-c-states/"/>
    <updated>2013-12-06T17:34:00+08:00</updated>
    <id>http://pyeh.github.io/blog/2013/12/06/intel-mide-cpuidle-intel-idle-processor-driver-and-new-c-states</id>
    <content type="html"><![CDATA[<p>When there is nothing left to do, the CPUs will go into the idle state to wait until it is needed again. These idle modes, which go by names like &ldquo;C states,&rdquo; vary in the amount of power saved, but also in the amount of ancillary information which may be lost and the amount of time required to get back into a fully-functional mode.</p>

<p>Intel_idle driver actually performs idle state power management, and regsiters into existing CPU Idle subsystem and extends driver for Merrifield CPU (Slivermont). It also introduces a new platform idle states C7x deeper than traditional C6 state. The overall idea is that CPU C7x-states are extended to devices and rest of the platform, hence puting the platform to S0ix states.</p>

<p>On intel mid platform (Merrifield), the PMU driver communicates with the Intel_idle driver, platform device drivers, pci drivers, and the PMU firmwares (NC: Punit, SC:SCU) to coordinate platform power state transitions.</p>

<ol>
<li><p>The PMU driver provides a platform-specific callback to the Intel_idle driver so that long periods of idleness can be extended to the entire platform.</p>

<ul>
<li>soc_s0ix_idle()&ndash;>enter_s0ix_state() // intel_idle driver defined at /drivers/idle/intel_idle.c</li>
<li>mid_s0ix_enter()            // PMU driver defined at /arch/x86/platform/intel-mid/intel_soc_pmu.c</li>
</ul>


<p> I could find out cpuidle_state->.enter() associated with soc_s0ix_idle() for C6 state on medfield platform, and the call stack looks like as above shown. However, I don&rsquo;t figure out the similar assoication between intel_idle and pmu driver on merrifield platform. I am curious if this statement is also appliciable on merrifield platform ??</p></li>
<li><p>Based on hint of idleness, the PMU driver extends CPU idleness to the reset of the platform via standard Linux PM_QoS, Runtime PM, and PCI PM calls.</p></li>
<li><p>Once the CPU and devices are all idle, the PMU driver programs the North and South Complex PMUs to implement the required power transitiion for S0ix to eumlate C7-x states.</p></li>
</ol>


<p>Here is very good reference to understand the cpuidle governor and subsystem
<a href="http://lwn.net/Articles/384146/">http://lwn.net/Articles/384146/</a> before digging into how intel_idle driver fits with current cpuilde intrastructure.</p>

<p>The below is code trace for intel_idle driver located at &ldquo;/drivers/idle/intel_idle.c&rdquo;
&ndash; Comment in intel_idle driver
<code>
/*
 * intel_idle is a cpuidle driver that loads on specific Intel processors
 * in lieu of the legacy ACPI processor_idle driver.  The intent is to
 * make Linux more efficient on these processors, as intel_idle knows
 * more than ACPI, as well as make Linux more immune to ACPI BIOS bugs.
 */
</code>
&ndash; Driver Init.</p>

<pre><code>* intel_idle_probe() starts to match current CPU with array of x86_cpu_ids via and assign corresponding default cpuidle C-state table  
* intel_idle_cpuidle_driver_init() checks real C-state table and update its state when needed (eg, target_residency)
* register the intel_dile driver with the cpudile subsystem through cpuidle_register_driver()  
* intel_idle_cpu_init() allocates, initializes, and registers cpuidle_device for each CPU
* register cpu_hotplug_notifer to know about CPUs going up/dow
</code></pre>

<p>``` c intel_idle_init()
static int __init intel_idle_init(void)                                                                                           <br/>
{</p>

<pre><code>    int retval, i;

    /* Do not load intel_idle at all for now if idle= is passed */
    if (boot_option_idle_override != IDLE_NO_OVERRIDE)
            return -ENODEV;

    retval = intel_idle_probe();
    if (retval)
            return retval;

    intel_idle_cpuidle_driver_init();
    retval = cpuidle_register_driver(&amp;intel_idle_driver);
    if (retval) {
            struct cpuidle_driver *drv = cpuidle_get_driver();
            printk(KERN_DEBUG PREFIX "intel_idle yielding to %s",
                    drv ? drv-&gt;name : "none");
            return retval;
    }

    intel_idle_cpuidle_devices = alloc_percpu(struct cpuidle_device);
    if (intel_idle_cpuidle_devices == NULL)
            return -ENOMEM;

    for_each_online_cpu(i) {
            retval = intel_idle_cpu_init(i);
            if (retval) {
                    cpuidle_unregister_driver(&amp;intel_idle_driver);
                    return retval;
            }

            if (platform_is(INTEL_ATOM_BYT)) {
                    /* Disable automatic core C6 demotion by PUNIT */
                    if (wrmsr_on_cpu(i, CLPU_CR_C6_POLICY_CONFIG,
                                    DISABLE_CORE_C6_DEMOTION, 0x0))
                            pr_err("Error to disable core C6 demotion");
                    /* Disable automatic module C6 demotion by PUNIT */
                    if (wrmsr_on_cpu(i, CLPU_MD_C6_POLICY_CONFIG,
                                    DISABLE_MODULE_C6_DEMOTION, 0x0))
                            pr_err("Error to disable module C6 demotion");
            }
    }
    register_cpu_notifier(&amp;cpu_hotplug_notifier);

    return 0;
</code></pre>

<p>}
```</p>

<ul>
<li>Default cpuidle C-states for merrifield

<ul>
<li>&ldquo;flags&rdquo; field describes the characteristics of this sleep state

<ul>
<li>CPUIDLE_FLAG_TIME_VALID should be set if it is possible to accurately measure the amount of time spent in this particular idle state.</li>
<li>CPUIDLE_FLAG_TLB_FLUSHED is set to inidicate the HW flushes the TLB for this state.</li>
<li>MWAIT takes an 8-bit &ldquo;hint&rdquo; in EAX &ldquo;suggesting&rdquo; the C-state (top nibble) and sub-state (bottom nibble). 0x00 means &ldquo;MWAIT(C1)&rdquo;, 0x10 means &ldquo;MWAIT(C2)&rdquo; etc.</li>
</ul>
</li>
<li>&ldquo;exit_latency&rdquo; in US says how long it takes to get back to a fully functional state.</li>
<li>&ldquo;target_residency&rdquo; in US is the minimum amount of time the processor should spend in this state to make the transition worth the effort.</li>
<li>The &ldquo;enter()&rdquo; function will be called when the current governor decides to put the CPU into the given state<br/>
``` c Merrifield CPUidle C states

<h1>if defined(CONFIG_REMOVEME_INTEL_ATOM_MRFLD_POWER)</h1>

static struct cpuidle_state mrfld_cstates[CPUIDLE_STATE_MAX] = {
  { /<em> MWAIT C1 </em>/
          .name = &ldquo;C1-ATM&rdquo;,
          .desc = &ldquo;MWAIT 0x00&rdquo;,
          .flags = MWAIT2flg(0x00) | CPUIDLE_FLAG_TIME_VALID,
          .exit_latency = 1,
          .target_residency = 4,
          .enter = &amp;intel_idle },
  { /<em> MWAIT C4 </em>/
          .name = &ldquo;C4-ATM&rdquo;,
          .desc = &ldquo;MWAIT 0x30&rdquo;,
          .flags = MWAIT2flg(0x30) | CPUIDLE_FLAG_TIME_VALID | CPUIDLE_FLAG_TLB_FLUSHED,
          .exit_latency = 100,
          .target_residency = 400,
          .enter = &amp;intel_idle },
  { /<em> MWAIT C6 </em>/
          .name = &ldquo;C6-ATM&rdquo;,
          .desc = &ldquo;MWAIT 0x52&rdquo;,
          .flags = MWAIT2flg(0x52) | CPUIDLE_FLAG_TIME_VALID | CPUIDLE_FLAG_TLB_FLUSHED,
          .exit_latency = 140,
          .target_residency = 560,
          .enter = &amp;intel_idle },
  { /<em> MWAIT C7-S0i1 </em>/
          .name = &ldquo;S0i1-ATM&rdquo;,
          .desc = &ldquo;MWAIT 0x60&rdquo;,
          .flags = MWAIT2flg(0x60) | CPUIDLE_FLAG_TIME_VALID | CPUIDLE_FLAG_TLB_FLUSHED,
          .exit_latency = 1200,
          .target_residency = 4000,
          .enter = &amp;intel_idle },
  { /<em> MWAIT C9-S0i3 </em>/
          .name = &ldquo;S0i3-ATM&rdquo;,
          .desc = &ldquo;MWAIT 0x64&rdquo;,
          .flags = MWAIT2flg(0x64) | CPUIDLE_FLAG_TIME_VALID | CPUIDLE_FLAG_TLB_FLUSHED,
          .exit_latency = 10000,
          .target_residency = 20000,
          .enter = &amp;intel_idle },
  {
          .enter = NULL }
};

<h1>else</h1>

<h1>define mrfld_cstates atom_cstates</h1>

<h1>endif</h1></li>
</ul>
</li>
</ul>


<p>```
The actual performs given C-state transitions implemented by intel_idle(). As we saw, this is done through &ldquo;enter()&rdquo; functions associated with each state. The decision as to which idle state makes sense in a given situation is very much a policy issue implemented by the cpuidle &ldquo;governors&rdquo;.</p>

<p>A call to enter() is a request from the current governor to put the CPU associated with dev into the given state. Note that enter() is free to choose a different state if there is a good reason to do so, but it should store the actual state used in the device&rsquo;s last_state field.
``` c intel_idle
/<em><em>                                                                                                                               <br/>
 * intel_idle
 * @dev: cpuidle_device
 * @drv: cpuidle driver
 * @index: index of cpuidle state
 *
 * Must be called under local_irq_disable().
 </em>/
static int intel_idle(struct cpuidle_device </em>dev,</p>

<pre><code>            struct cpuidle_driver *drv, int index)
</code></pre>

<p>{</p>

<pre><code>    unsigned long ecx = 1; /* break on interrupt flag */
    struct cpuidle_state *state = &amp;drv-&gt;states[index];
    unsigned long eax = flg2MWAIT(state-&gt;flags);
    unsigned int cstate;
    int cpu = smp_processor_id();
</code></pre>

<h1>if (defined(CONFIG_REMOVEME_INTEL_ATOM_MRFLD_POWER) &amp;&amp; \</h1>

<pre><code>    defined(CONFIG_PM_DEBUG))
    {
            /* Get Cstate based on ignore table from PMU driver */
            unsigned int ncstate;
            cstate =
            (((eax) &gt;&gt; MWAIT_SUBSTATE_SIZE) &amp; MWAIT_CSTATE_MASK) + 1;
            ncstate = pmu_get_new_cstate(cstate, &amp;index);
            eax     = flg2MWAIT(drv-&gt;states[index].flags);
    }
</code></pre>

<h1>endif</h1>

<pre><code>    cstate = (((eax) &gt;&gt; MWAIT_SUBSTATE_SIZE) &amp; MWAIT_CSTATE_MASK) + 1;

    /*
     * leave_mm() to avoid costly and often unnecessary wakeups
     * for flushing the user TLB's associated with the active mm.
     */
    if (state-&gt;flags &amp; CPUIDLE_FLAG_TLB_FLUSHED)
            leave_mm(cpu);

    if (!(lapic_timer_reliable_states &amp; (1 &lt;&lt; (cstate))))
            clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &amp;cpu);

    if (!need_resched()) {

            __monitor((void *)&amp;current_thread_info()-&gt;flags, 0, 0);
            smp_mb();
            if (!need_resched())
                    __mwait(eax, ecx);
</code></pre>

<h1>if defined(CONFIG_REMOVEME_INTEL_ATOM_MDFLD_POWER) || \</h1>

<pre><code>    defined(CONFIG_REMOVEME_INTEL_ATOM_CLV_POWER)
            if (!need_resched() &amp;&amp; is_irq_pending() == 0)
                    __get_cpu_var(update_buckets) = 0;
</code></pre>

<h1>endif</h1>

<pre><code>    }

    if (!(lapic_timer_reliable_states &amp; (1 &lt;&lt; (cstate))))
            clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &amp;cpu);

    return index;
</code></pre>

<p>}</p>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Intel_mid: Android power management flow for suspend/resume using S0i3 implementation]]></title>
    <link href="http://pyeh.github.io/blog/2013/12/04/intel-mid-android-power-management-flow-for-suspend-slash-resume-using-s0i3-implementation/"/>
    <updated>2013-12-04T11:19:00+08:00</updated>
    <id>http://pyeh.github.io/blog/2013/12/04/intel-mid-android-power-management-flow-for-suspend-slash-resume-using-s0i3-implementation</id>
    <content type="html"><![CDATA[<p>The main PMU driver (arch/x86/platform/intel-mid/intel_soc_pmu.c) hooks to support Linux PM suspend/resume flows as follows.<br/>
The S0ix states are low-power active idle states that platform can be transitioned into.
&ndash; Register PMU driver as PCI device<br/>
The PMU driver registers mid_suspend_ops via suspend_set_ops().
```c mid_pci_register_init
/<em>*
 * mid_pci_register_init &ndash; register the PMU driver as PCI device
 </em>/
static struct pci_driver driver = {</p>

<pre><code>    .name = PMU_DRV_NAME,
    .id_table = mid_pm_ids,
    .probe = mid_pmu_probe,
    .remove = mid_pmu_remove,
    .shutdown = mid_pmu_shutdown
</code></pre>

<p>};</p>

<p>static int __init mid_pci_register_init(void)
{</p>

<pre><code>    int ret;

    mid_pmu_cxt = kzalloc(sizeof(struct mid_pmu_dev), GFP_KERNEL);

    if (mid_pmu_cxt == NULL)
            return -ENOMEM;

    mid_pmu_cxt-&gt;s3_restrict_qos =
            kzalloc(sizeof(struct pm_qos_request), GFP_KERNEL);
    if (mid_pmu_cxt-&gt;s3_restrict_qos) {
            pm_qos_add_request(mid_pmu_cxt-&gt;s3_restrict_qos,
                     PM_QOS_CPU_DMA_LATENCY, PM_QOS_DEFAULT_VALUE);
    } else {
            return -ENOMEM;
    }

    init_nc_device_states();

    mid_pmu_cxt-&gt;nc_restrict_qos =
            kzalloc(sizeof(struct pm_qos_request), GFP_KERNEL);
    if (mid_pmu_cxt-&gt;nc_restrict_qos == NULL)
            return -ENOMEM;

    /* initialize the semaphores */
    sema_init(&amp;mid_pmu_cxt-&gt;scu_ready_sem, 1);

    /* registering PCI device */
    ret = pci_register_driver(&amp;driver);
    suspend_set_ops(&amp;mid_suspend_ops);

    return ret;
</code></pre>

<p>}
<code>
- mid_suspend_enter() performs the required S3 state over standby_enter()  
check_nc_sc_status() hooked by mrfld_nc_sc_status_check (defind at arch/x86/platform/intel-mid/intel_soc_mrfld.c) is to check north complex (NC) and soutch complex (SC) device status. Return true if all NC and SC devices are in D0i3.
</code> c mid_suspend_enter()
static const struct platform_suspend_ops mid_suspend_ops = {</p>

<pre><code>    .begin = mid_suspend_begin,
    .valid = mid_suspend_valid,
    .prepare = mid_suspend_prepare,
    .prepare_late = mid_suspend_prepare_late,
    .enter = mid_suspend_enter,
    .end = mid_suspend_end,
</code></pre>

<p>};</p>

<p>static int mid_suspend_enter(suspend_state_t state)                                                                               <br/>
{</p>

<pre><code>    int ret;

    if (state != PM_SUSPEND_MEM)
            return -EINVAL;

    /* one last check before entering standby */
    if (pmu_ops-&gt;check_nc_sc_status) {
            if (!(pmu_ops-&gt;check_nc_sc_status())) {
                    trace_printk("Device d0ix status check failed! Aborting Standby entry!\n");
                    WARN_ON(1);
            }
    }

    trace_printk("s3_entry\n");
    ret = standby_enter();
    trace_printk("s3_exit %d\n", ret);
    if (ret != 0)
            dev_dbg(&amp;mid_pmu_cxt-&gt;pmu_dev-&gt;dev,
                            "Failed to enter S3 status: %d\n", ret);

    return ret;
</code></pre>

<p>}</p>

<p>```</p>

<ul>
<li>standby_enter() performs requried S3 state

<ul>
<li>mid_state_to_sys_state() maps power states to driver&rsquo;s internal indexes.</li>
<li>mid_s01x_enter() performs required S3 state</li>
<li>__mwait(mid_pmu_cxt->s3_hint, 1) is issued with a hint to enter S0i3(S3 emulation using S0i3, as I observed that MRFLD_S3_HINT with 0x64 is same as MID_S0I3_STATE with 0x64). When both core issue an mwait C7, it is a hint provided by the idle driver to enter an S0ix state.</li>
<li><p>This triggers S0i3 entry, but the decision and policy is selected by SCU FW.
``` c standby_enter()
static int standby_enter(void)
{
  u32 temp = 0;
  int s3_state = mid_state_to_sys_state(MID_S3_STATE);</p>

<p>  if (mid_s0ix_enter(MID_S3_STATE) != MID_S3_STATE) {
          pmu_set_s0ix_complete();
          return -EINVAL;
  }</p>

<p>  /<em> time stamp for end of s3 entry </em>/
  time_stamp_for_sleep_state_latency(s3_state, false, true);</p>

<p>  <strong>monitor((void *) &amp;temp, 0, 0);
  smp_mb();
  </strong>mwait(mid_pmu_cxt->s3_hint, 1);</p>

<p>  /<em> time stamp for start of s3 exit </em>/
  time_stamp_for_sleep_state_latency(s3_state, true, false);</p>

<p>  pmu_set_s0ix_complete();</p>

<p>  /<em>set wkc to appropriate value suitable for s0ix</em>/
  writel(mid_pmu_cxt->ss_config->wake_state.wake_enable[0],
                 &amp;mid_pmu_cxt->pmu_reg->pm_wkc[0]);
  writel(mid_pmu_cxt->ss_config->wake_state.wake_enable[1],
                 &amp;mid_pmu_cxt->pmu_reg->pm_wkc[1]);</p>

<p>  mid_pmu_cxt->camera_off = 0;
  mid_pmu_cxt->display_off = 0;</p>

<p>  if (platform_is(INTEL_ATOM_MRFLD))
          up(&amp;mid_pmu_cxt->scu_ready_sem);</p>

<p>  return 0;
}
```</p></li>
</ul>
</li>
<li>mid_s01x_enter()

<ul>
<li>pmu_prepare_wake() will mask wakeup from AONT timers for s3. If s3 is aborted for any reason, we don&rsquo;t want to leave AONT timers masked until next suspend, otherwise if next to happen is s0ix, no timer could wakeup SoC from s0ix and we might miss to kick the kernel watchdog.</li>
<li><p>enter() hooked by mrfld_pmu_enter (defined at arch/x86/platform/intel-mid/intel_soc_mrfld.c). Compared to Medfield and Covertail platform, PM_CMD is not required to send to SCU.
``` c mid_s01x_enter()
int mid_s0ix_enter(int s0ix_state)
{
  int ret = 0;</p>

<p>  if (unlikely(!pmu_ops || !pmu_ops->enter))
          goto ret;</p>

<p>  /* check if we can acquire scu_ready_sem</p>

<ul>
<li> if we are not able to then do a c6 */
if (down_trylock(&amp;mid_pmu_cxt->scu_ready_sem))
      goto ret;</li>
</ul>


<p>  /<em> If PMU is busy, we&rsquo;ll retry on next C6 </em>/
  if (unlikely(_pmu_read_status(PMU_BUSY_STATUS))) {
          up(&amp;mid_pmu_cxt->scu_ready_sem);
          pr_debug(&ldquo;mid_pmu_cxt->scu_read_sem is up\n&rdquo;);
          goto ret;
  }</p>

<p>  pmu_prepare_wake(s0ix_state);</p>

<p>  /<em> no need to proceed if schedule pending </em>/
  if (unlikely(need_resched())) {
          pmu_stat_clear();
          up(&amp;mid_pmu_cxt->scu_ready_sem);
          goto ret;
  }</p>

<p>  /<em> entry function for pmu driver ops </em>/
  if (pmu_ops->enter(s0ix_state))
          ret = s0ix_state;</p></li>
</ul>
</li>
</ul>


<p>ret:</p>

<pre><code>    return ret;
</code></pre>

<p>}</p>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Intel_mid: Introduction of android watchdogs and kernel watchdogs]]></title>
    <link href="http://pyeh.github.io/blog/2013/12/02/intel-mid-introduction-of-android-watchdog-and-kernel-watchdog/"/>
    <updated>2013-12-02T17:29:00+08:00</updated>
    <id>http://pyeh.github.io/blog/2013/12/02/intel-mid-introduction-of-android-watchdog-and-kernel-watchdog</id>
    <content type="html"><![CDATA[<p>Watchdogs monitor software and hardware device and prevent whole system from hanging. After looking into android BSP running on intel mid, merrifield, platform, I will try to classfiy watchdog into the following types. The first two belong to native Android supporting, and the last three are specified to intel-mid platform.</p>

<h2>1. Android framework&rsquo;s Java* watchdog</h2>

<p>Deal with cases when any of the following locks is held for more than a minute or when ServerThread is busy.</p>

<p>&mdash; ThermalManagerService<br/>
&mdash; PowerManagerService<br/>
&mdash; WindowMangerService<br/>
&mdash; MountService<br/>
&mdash; NetworkManagementService<br/>
&mdash; ActivityMangerService</p>

<p>If one of above services hangs for one minute, the java watchdog kills it and results in restarting android&rsquo;s framework by killing the SystemServer.</p>

<h2>2. Device-critical services</h2>

<p>Critical services are declared as &ldquo;critical&rdquo; in the corresponding rc files (eg, ueventd, servicemanager). If critical services exist or crash more than four times in four minutes, the device will reboot into recovery mode. This feature is handled by the init process.</p>

<h2>3. Kernel watchdog leads to COLD_RESET</h2>

<p>The kernel watchdog prvents the operating system from hanging. The System Control Unit (SCU firmware) resets the platform when the kernel cannot schedule the watchdog daemon (/usr/bin/ia_watchdogd).</p>

<p>The driver located at /drivers/watchdog/intel_scu_watchdog_evo.c provides a /dev/watchdog device to access the kernel watchdog and ioctls to configure the timer. Since the SCU provides the functionality, all access to watchdog features are routed to the SCU via an IPC (see more PIC regitrations at arch/x86/platform/intel-mid/intel_mid_scu.c)</p>

<ul>
<li>Init code installs the driver
``` c watchdog_rpmsg_init()
static struct rpmsg_driver watchdog_rpmsg = {
      .drv.name       = KBUILD_MODNAME,
      .drv.owner      = THIS_MODULE,
      .id_table       = watchdog_rpmsg_id_table,
      .probe          = watchdog_rpmsg_probe,
      .callback       = watchdog_rpmsg_cb,
      .remove         = watchdog_rpmsg_remove,
};</li>
</ul>


<p>static int __init watchdog_rpmsg_init(void)
{</p>

<pre><code>    if (intel_mid_identify_cpu() == INTEL_MID_CPU_CHIP_TANGIER)
            return register_rpmsg_driver(&amp;watchdog_rpmsg);
    else {                                                                                                                      
            pr_err("%s: watchdog driver: bad platform\n", __func__);
            return -ENODEV;
    }
</code></pre>

<p>}</p>

<h1>ifdef MODULE</h1>

<p>module_init(watchdog_rpmsg_init);</p>

<h1>else</h1>

<p>rootfs_initcall(watchdog_rpmsg_init);</p>

<h1>endif</h1>

<p>```</p>

<ul>
<li><p>create the /dev/watchdog only if the disabled_kernel_watchdog module parameter is not set. It gets the timer&rsquo;s configuration, registers reboot notifier, registers dump handler to irq#15, and adds sysfs/debugfs entries.
``` c watchdog_rpmsg_probe()&ndash;>intel_scu_watchdog_init()
/<em> Init code </em>/
static int intel_scu_watchdog_init(void)
{
      int ret = 0;</p>

<pre><code>  watchdog_device.normal_wd_action   = SCU_COLD_RESET_ON_TIMEOUT;
  watchdog_device.reboot_wd_action   = SCU_COLD_RESET_ON_TIMEOUT;
  watchdog_device.shutdown_wd_action = SCU_COLD_OFF_ON_TIMEOUT;
</code></pre></li>
</ul>


<h1>ifdef CONFIG_DEBUG_FS</h1>

<pre><code>    watchdog_device.panic_reboot_notifier = false;
</code></pre>

<h1>endif /<em> CONFIG_DEBUG_FS </em>/</h1>

<pre><code>    /* Initially, we are not in shutdown mode */
    watchdog_device.shutdown_flag = false;

    /* Check timeouts boot parameter */
    if (check_timeouts(pre_timeout, timeout)) {
            pr_err("%s: Invalid timeouts\n", __func__);
            return -EINVAL;
    }

    /* Reboot notifier */
    watchdog_device.reboot_notifier.notifier_call = reboot_notifier;
    watchdog_device.reboot_notifier.priority = 1;
    ret = register_reboot_notifier(&amp;watchdog_device.reboot_notifier);
    if (ret) {
            pr_crit("cannot register reboot notifier %d\n", ret);
            goto error_stop_timer;
    }

    /* Do not publish the watchdog device when disable (TO BE REMOVED) */
    if (!disable_kernel_watchdog) {
            watchdog_device.miscdev.minor = WATCHDOG_MINOR;
            watchdog_device.miscdev.name = "watchdog";
            watchdog_device.miscdev.fops = &amp;intel_scu_fops;

            ret = misc_register(&amp;watchdog_device.miscdev);
            watchdog_device.miscdev.fops = &amp;intel_scu_fops;

            ret = misc_register(&amp;watchdog_device.miscdev);
            if (ret) {
                    pr_crit("Cannot register miscdev %d err =%d\n",
                            WATCHDOG_MINOR, ret);
                    goto error_reboot_notifier;
            }
    }

    /* MSI #15 handler to dump registers */
    handle_mrfl_dev_ioapic(EXT_TIMER0_MSI);
    ret = request_irq((unsigned int)EXT_TIMER0_MSI,
            watchdog_warning_interrupt,
            IRQF_SHARED|IRQF_NO_SUSPEND, "watchdog",
            &amp;watchdog_device);
    if (ret) {
            pr_err("error requesting warning irq %d\n",
                   EXT_TIMER0_MSI);
            pr_err("error value returned is %d\n", ret);
            goto error_misc_register;
    }
</code></pre>

<h1>ifdef CONFIG_INTEL_SCU_SOFT_LOCKUP</h1>

<pre><code>    init_timer(&amp;softlock_timer);
</code></pre>

<h1>endif</h1>

<pre><code>    if (disable_kernel_watchdog) {
            pr_err("%s: Disable kernel watchdog\n", __func__);

            /* Make sure timer is stopped */
            ret = watchdog_stop();
            if (ret != 0)
                    pr_debug("cant disable timer\n");
    }
</code></pre>

<h1>ifdef CONFIG_DEBUG_FS</h1>

<pre><code>    ret = create_debugfs_entries();  
    if (ret) {
            pr_err("%s: Error creating debugfs entries\n", __func__);
            goto error_debugfs_entry;
    }
</code></pre>

<h1>endif</h1>

<pre><code>    watchdog_device.started = false;

    ret = create_watchdog_sysfs_files();
    if (ret) {
            pr_err("%s: Error creating debugfs entries\n", __func__);
            goto error_sysfs_entry;
    }

    return ret;
</code></pre>

<p>error_sysfs_entry:</p>

<pre><code>    /* Nothing special to do */
</code></pre>

<h1>ifdef CONFIG_DEBUG_FS</h1>

<p>error_debugfs_entry:</p>

<pre><code>    /* Remove entries done by create function */
</code></pre>

<h1>endif</h1>

<p>error_misc_register:</p>

<pre><code>    misc_deregister(&amp;watchdog_device.miscdev);
</code></pre>

<p>error_reboot_notifier:</p>

<pre><code>    unregister_reboot_notifier(&amp;watchdog_device.reboot_notifier);
</code></pre>

<p>error_stop_timer:</p>

<pre><code>    watchdog_stop();

    return ret;
</code></pre>

<p>}
```</p>

<ul>
<li><p>interrupt handler related to pre-timeout dumps kernel backtraces.
``` c watchdog_warning_interrupt
/<em> warning interrupt handler </em>/
static irqreturn_t watchdog_warning_interrupt(int irq, void *dev_id)
{
      pr_warn(&ldquo;[SHTDWN] %s, WATCHDOG TIMEOUT!\n&rdquo;, <strong>func</strong>);</p>

<pre><code>  /* Let's reset the platform after dumping some data */ 
  trigger_all_cpu_backtrace(); 
  panic("Kernel Watchdog"); 

  /* This code should not be reached */ 
  return IRQ_HANDLED; 
</code></pre>

<p>}</p></li>
</ul>


<p>```</p>

<ul>
<li><p>When power transisition happens, reboot_notifier is called for re-configuring watchdog timeouts and its default behavior.
COLD_RESET is set to reboot, and COLD_OFF is set to poewr halt and off. In case of a stucking rebooting or shutdown procedure, the platform will still could execute reset or power-off seperately.
``` c reboot_notifier
/<em> Reboot notifier </em>/
static int reboot_notifier(struct notifier_block <em>this,
                         unsigned long code,
                         void </em>another_unused)
{
      int ret;</p>

<pre><code>  if (code == SYS_RESTART || code == SYS_HALT || code == SYS_POWER_OFF) {
          pr_warn("Reboot notifier\n");

          if (watchdog_set_appropriate_timeouts())
                  pr_crit("reboot notifier cant set time\n");

          switch (code) {
          case SYS_RESTART:
                  ret = watchdog_set_reset_type(
                          watchdog_device.reboot_wd_action);
                  break;

          case SYS_HALT:
          case SYS_POWER_OFF:
                  ret = watchdog_set_reset_type(
                          watchdog_device.shutdown_wd_action);
                  break;
          }
          if (ret)
                  pr_err("%s: could not set reset type\n", __func__);
</code></pre></li>
</ul>


<h1>ifdef CONFIG_DEBUG_FS</h1>

<pre><code>            /* debugfs entry to generate a BUG during
            any shutdown/reboot call */
            if (watchdog_device.panic_reboot_notifier)
                    BUG();
</code></pre>

<h1>endif</h1>

<pre><code>            /* Don't do instant reset on close */
            reset_on_release = false;

            /* Kick once again */
            if (disable_kernel_watchdog == false) {
                    ret = watchdog_keepalive();
                    if (ret)
                            pr_warn("%s: no keep alive\n", __func__);

                    /* Don't allow any more keep-alives */
                    watchdog_device.shutdown_flag = true;
            }
    }
    return NOTIFY_DONE;
</code></pre>

<p>}
```</p>

<h2>4. Userspace watchdog daemon</h2>

<p>Source codes are located at /hardware/ia_watchdog/watchdog_daemon folder) and target location is /usr/bin/ia_watchdogd. This daemon is declared as one-shot service in the rc file (init.watchdog.rc) and perform the following steps:</p>

<p>&mdash; open the watchdog device /dev/watchdog.<br/>
&mdash; configure the pre_timeout with 75 seconds and timeout with 90 seconds.<br/>
&mdash; Loop forever. In the loop, kick the watchdog device (by writing to &lsquo;R&rsquo; to /dev/watchdog) every 60 seconds.</p>

<h2>5. SCU watchdog leads to PLATFORM_RESET(deep reset)</h2>

<p>This prevents the platform stucking on SCU by issuing a PLATFORM_RESET because the interface between the SCU and PMIC is broken.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[intel_mid: Overview of Simple Platform Interface]]></title>
    <link href="http://pyeh.github.io/blog/2013/11/27/intel-mid-overview-of-simple-platform-interface/"/>
    <updated>2013-11-27T15:40:00+08:00</updated>
    <id>http://pyeh.github.io/blog/2013/11/27/intel-mid-overview-of-simple-platform-interface</id>
    <content type="html"><![CDATA[<p>Start to develop android kernel under X86 MID platform, merrifield.
I am a newbie for porting kernel to Atom-based platform but am familiar with linux kernel and device drivers.
Noticed that Intel Android BSP introudes Simple Firmware Interface (SFI) a method for platform firmware to export
static tables (I2C, SPI, GPIO) to the operation system.</p>

<p>Intel&rsquo;s newer Atom processors support SFI since &ldquo;Moorestown&rdquo; SoC and SFI implementation was merged into upstream kernel 2.6.32(<a href="http://lwn.net/Articles/340476">http://lwn.net/Articles/340476</a>)</p>

<p>Actually, below link descrbies the SFI and explains how does SFI related to ACPI and UEFI.
<a href="https://simplefirmware.org/faq">https://simplefirmware.org/faq</a></p>

<p>Besides, below patch sets are to refactor existing code and implement a flexible way to support multiple boards and devices.
<a href="https://lkml.org/lkml/2013/10/10/81">https://lkml.org/lkml/2013/10/10/81</a></p>

<p>/arch/x86/platform/intel-mid/intel_mid_sfi.c is SFI parsing implementation, and let me understand how get_gpio_by_name() works.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Android ram-console upstreaming]]></title>
    <link href="http://pyeh.github.io/blog/2013/11/08/android-ram-console-upstreaming/"/>
    <updated>2013-11-08T17:45:00+08:00</updated>
    <id>http://pyeh.github.io/blog/2013/11/08/android-ram-console-upstreaming</id>
    <content type="html"><![CDATA[<p>While dealing with board bring-up powered by Tegra5, starts to aware of this upstream change for ram console (aka /proc/last_kmsg). A working group, Android Upstreaming, comes from Linaro foundation to merge ram_console into pstore framework ( <a href="http://lwn.net/Articles/497881/">http://lwn.net/Articles/497881/</a>).</p>

<p>The Android Upstreaming team&rsquo;s mission is to reduce and eventually eliminate the differences between the upstream kernel and the Android kernel. The team works closely with Google and upstream kernel developers to find ways to implement Android required features in a way that meets the need of both communities.</p>

<p>There are currently two competing debug facilities to store kernel messages in a persistent storage: a generic pstore and Google&rsquo;s persistent_ram by Colin Cross. Not so long ago (<a href="https://lkml.org/lkml/2012/3/8/252">https://lkml.org/lkml/2012/3/8/252</a>) noticed by Greg KH@ARM Linux, it was decided to fix this situation. There is a buleprint registered by Linaro Linux to descrbie those debug facilities at <a href="https://blueprints.launchpad.net/linux-linaro/+spec/android-ram-console">https://blueprints.launchpad.net/linux-linaro/+spec/android-ram-console</a></p>

<p>To follow up android upstreaming&rsquo;s works, I remove legacy driver supporting for ram console and persistent ram and switch new pstore framework. In other words, start to looking into /sys/fs/pstore/console-ramoops as we used to did analysis on /proc/last_kmsg for dying moment across system reboot.</p>

<p>My works were merged into Asus internal development branch jb-mr2-t50-k3.10, but not opened yet. We could refer to <a href="https://android.googlesource.com/kernel/tegra/+/android-tegra-3.10">https://android.googlesource.com/kernel/tegra/+/android-tegra-3.10</a> for further reference.</p>
]]></content>
  </entry>
  
</feed>
